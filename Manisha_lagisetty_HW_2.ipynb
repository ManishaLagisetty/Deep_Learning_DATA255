{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 1. Follow along with the tutorial to gain an understanding of the process.**"
      ],
      "metadata": {
        "id": "Y8tHSvfmHtL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary Libraries**"
      ],
      "metadata": {
        "id": "1poD9FX5H0e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3PVEM5JIH1dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 2. In a new .ipynb notebook, reproduce the results utilizing the \"QMNIST\" dataset.**"
      ],
      "metadata": {
        "id": "QMb70I_KH5xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading QMNIST dataset and preprocessing it\n",
        "QMNIST_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "QMNIST_train_dataset = torchvision.datasets.QMNIST(root='./data', train=True, download=True, transform=QMNIST_transform)\n",
        "QMNIST_test_dataset = torchvision.datasets.QMNIST(root='./data', train=False, download=True, transform=QMNIST_transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZPazttvH93q",
        "outputId": "2c5bd6d5-9e0f-4aa4-b8df-75111328fd8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/facebookresearch/qmnist/master/qmnist-train-images-idx3-ubyte.gz to ./data/QMNIST/raw/qmnist-train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9704059/9704059 [00:00<00:00, 63999115.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/QMNIST/raw/qmnist-train-images-idx3-ubyte.gz to ./data/QMNIST/raw\n",
            "Downloading https://raw.githubusercontent.com/facebookresearch/qmnist/master/qmnist-train-labels-idx2-int.gz to ./data/QMNIST/raw/qmnist-train-labels-idx2-int.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 463024/463024 [00:00<00:00, 12500730.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/QMNIST/raw/qmnist-train-labels-idx2-int.gz to ./data/QMNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/facebookresearch/qmnist/master/qmnist-test-images-idx3-ubyte.gz to ./data/QMNIST/raw/qmnist-test-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9742279/9742279 [00:00<00:00, 65054678.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/QMNIST/raw/qmnist-test-images-idx3-ubyte.gz to ./data/QMNIST/raw\n",
            "Downloading https://raw.githubusercontent.com/facebookresearch/qmnist/master/qmnist-test-labels-idx2-int.gz to ./data/QMNIST/raw/qmnist-test-labels-idx2-int.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 526800/526800 [00:00<00:00, 11378161.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/QMNIST/raw/qmnist-test-labels-idx2-int.gz to ./data/QMNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Data Loaders\n",
        "QMNIST_train_loader = DataLoader(QMNIST_train_dataset, batch_size=64, shuffle=True)\n",
        "QMNIST_test_loader = DataLoader(QMNIST_test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "wpmH7VztIap2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vsualizing the QMNIST training dataset\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.imshow(QMNIST_train_dataset[i][0].squeeze(), cmap='gray')\n",
        "    plt.title(f'Label: {QMNIST_train_dataset[i][1]}')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "OI7jXLrBJNhT",
        "outputId": "e90845e3-f1c3-4d8e-b2ba-ccdb53ce7980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 25 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing the QMNIST test dataset\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.imshow(QMNIST_test_dataset[i][0].squeeze(), cmap='gray')\n",
        "    plt.title(f'Label: {QMNIST_test_dataset[i][1]}')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "ZkQ2272jJ6sa",
        "outputId": "b379a703-92d3-4bdf-ce7c-46166e8a8982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 25 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 3. Report on the results in terms of prediction accuracy on the train and test datasets.**"
      ],
      "metadata": {
        "id": "AXuZlLsoKGbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Defining Neural Network Model**"
      ],
      "metadata": {
        "id": "LkN5hWxBNx6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class QMNIST_MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QMNIST_MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "i3ctVzp2KKCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Initializing the Neural Netwotk model**"
      ],
      "metadata": {
        "id": "DbfVFSX7N2bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QMNIST_model = QMNIST_MLP()"
      ],
      "metadata": {
        "id": "qqV7VXjzMsnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Defining Loss Function as Cross Entropy and Optimizer as Adam**\n",
        "* **Training the QMNIST neural Network**"
      ],
      "metadata": {
        "id": "l76moDV4N7fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the loss function and optimizer\n",
        "QMNIST_criterion = nn.CrossEntropyLoss()\n",
        "QMNIST_optimizer = optim.Adam(QMNIST_model.parameters(), lr=0.001)\n",
        "\n",
        "#Training the neural network\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    QMNIST_model.train()\n",
        "    running_loss = 0.0\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    for i, data in enumerate(QMNIST_train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        QMNIST_optimizer.zero_grad()\n",
        "        outputs = QMNIST_model(inputs)\n",
        "        loss = QMNIST_criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        QMNIST_optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        #Calculating the training dataset accuracy\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Calculate train accuracy\n",
        "    train_accuracy = correct_train / total_train\n",
        "    print(f'Epoch {epoch + 1}, Accuracy on Training Dataset: {train_accuracy}')\n",
        "\n",
        "print('Finished QMNIST Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tERJDDP_OFss",
        "outputId": "4dd5a124-b9ec-44f5-b1c7-3da84e971270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.0390153348445892\n",
            "Epoch 1, Batch 200, Loss: 0.4385501918196678\n",
            "Epoch 1, Batch 300, Loss: 0.3972736456990242\n",
            "Epoch 1, Batch 400, Loss: 0.35613451033830645\n",
            "Epoch 1, Batch 500, Loss: 0.3314203626662493\n",
            "Epoch 1, Batch 600, Loss: 0.31087343223392966\n",
            "Epoch 1, Batch 700, Loss: 0.28889388471841815\n",
            "Epoch 1, Batch 800, Loss: 0.2692469122260809\n",
            "Epoch 1, Batch 900, Loss: 0.25445860106498003\n",
            "Epoch 1, Accuracy on Training Dataset: 0.8799\n",
            "Epoch 2, Batch 100, Loss: 0.20158951323479413\n",
            "Epoch 2, Batch 200, Loss: 0.20427116695791483\n",
            "Epoch 2, Batch 300, Loss: 0.19844556726515294\n",
            "Epoch 2, Batch 400, Loss: 0.20524074032902717\n",
            "Epoch 2, Batch 500, Loss: 0.18480144754052164\n",
            "Epoch 2, Batch 600, Loss: 0.18319572370499373\n",
            "Epoch 2, Batch 700, Loss: 0.18089312624186277\n",
            "Epoch 2, Batch 800, Loss: 0.1691407470405102\n",
            "Epoch 2, Batch 900, Loss: 0.19728902414441107\n",
            "Epoch 2, Accuracy on Training Dataset: 0.9428666666666666\n",
            "Epoch 3, Batch 100, Loss: 0.1428232116624713\n",
            "Epoch 3, Batch 200, Loss: 0.14315339466556906\n",
            "Epoch 3, Batch 300, Loss: 0.13340531881898643\n",
            "Epoch 3, Batch 400, Loss: 0.1445637162029743\n",
            "Epoch 3, Batch 500, Loss: 0.15092569760978222\n",
            "Epoch 3, Batch 600, Loss: 0.13909905787557364\n",
            "Epoch 3, Batch 700, Loss: 0.12424528069794177\n",
            "Epoch 3, Batch 800, Loss: 0.1351991686783731\n",
            "Epoch 3, Batch 900, Loss: 0.1429640543833375\n",
            "Epoch 3, Accuracy on Training Dataset: 0.95625\n",
            "Epoch 4, Batch 100, Loss: 0.10816381886601448\n",
            "Epoch 4, Batch 200, Loss: 0.11113069950602948\n",
            "Epoch 4, Batch 300, Loss: 0.11094400651752949\n",
            "Epoch 4, Batch 400, Loss: 0.11393365998752415\n",
            "Epoch 4, Batch 500, Loss: 0.10967038303613663\n",
            "Epoch 4, Batch 600, Loss: 0.11195636023767293\n",
            "Epoch 4, Batch 700, Loss: 0.11254375809803605\n",
            "Epoch 4, Batch 800, Loss: 0.114257927024737\n",
            "Epoch 4, Batch 900, Loss: 0.11595935001969337\n",
            "Epoch 4, Accuracy on Training Dataset: 0.9664\n",
            "Epoch 5, Batch 100, Loss: 0.09725922838784755\n",
            "Epoch 5, Batch 200, Loss: 0.08770083640702069\n",
            "Epoch 5, Batch 300, Loss: 0.09807912334799766\n",
            "Epoch 5, Batch 400, Loss: 0.09483060896396638\n",
            "Epoch 5, Batch 500, Loss: 0.09255938426591456\n",
            "Epoch 5, Batch 600, Loss: 0.09880578543059527\n",
            "Epoch 5, Batch 700, Loss: 0.09490118257701396\n",
            "Epoch 5, Batch 800, Loss: 0.08671622053720057\n",
            "Epoch 5, Batch 900, Loss: 0.09443137569352984\n",
            "Epoch 5, Accuracy on Training Dataset: 0.97135\n",
            "Finished QMNIST Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Evaluating the Test Model**"
      ],
      "metadata": {
        "id": "WsbjBTubO29p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QMNIST_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in QMNIST_test_loader:\n",
        "        images, labels = data\n",
        "        outputs = QMNIST_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on testing dataset: { correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afuwwMoZOO0u",
        "outputId": "59de54b4-d121-4aa2-bf7b-6fbd97fca927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing dataset: 0.96315%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training runs through the dataset for 5 epochs, adjusting the model's parameters using the Adam optimizer and cross-entropy loss function. As it trains, the loss reduces gradually, indicating progress in learning. Also, the training accuracy steadily improves, reaching around 97.13% after 5 epochs and showing that the model can correctly classify images in the training set.\n",
        "\n",
        "This increase in accuracy suggests that the model effectively learns from patterns within the dataset. In addition, with a testing accuracy of about 96.35%, it demonstrates how well this model generalizes to unseen data. The choice of hyperparameters—such as learning rate, optimizer selection, and model architecture (number of layers and neurons)—significantly impacts both training process and performance observed."
      ],
      "metadata": {
        "id": "GYo1j5liQu3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 4-6**\n",
        "\n",
        "**Choose one of the proposed modifications below:**\n",
        "\n",
        "**Modify the model based on the chosen method and train**\n",
        "\n",
        "**Report on the results of the modified model and if it matches your hypothesis**\n",
        "\n"
      ],
      "metadata": {
        "id": "P48vJtVAQ03b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Add another Dense layer of 128 nodes**"
      ],
      "metadata": {
        "id": "bShJfgboQ7Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModifiedMLP1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModifiedMLP1, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)  #Additional layer OF 128 NODES\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "#Initializing the modified neural network propsed modified model 1\n",
        "modified_model1 = ModifiedMLP1()\n",
        "\n",
        "#Defining the loss function and optimizer for modified model 1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(modified_model1.parameters(), lr=0.001)\n",
        "\n",
        "#Training the modified neural network model 1\n",
        "for epoch in range(num_epochs):\n",
        "    modified_model1.train()\n",
        "    running_loss = 0.0\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    for i, data in enumerate(QMNIST_train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = modified_model1(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Calculate train accuracy\n",
        "    train_accuracy = correct_train / total_train\n",
        "    print(f'Epoch {epoch + 1}, Accuracy on Training dataset: {train_accuracy}')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "#Evaluating the modified model 1 on test set\n",
        "modified_model1.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in QMNIST_test_loader:\n",
        "        images, labels = data\n",
        "        outputs = modified_model1(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "modified_test_accuracy1 = correct / total\n",
        "print(f'Accuracy on testing dataset (modified model - 1): {modified_test_accuracy1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naNcqlOkWfyP",
        "outputId": "6c3c6d25-d659-48cf-b2ea-e96b77f801d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.0709230706095696\n",
            "Epoch 1, Batch 200, Loss: 0.45688184842467305\n",
            "Epoch 1, Batch 300, Loss: 0.39139827966690066\n",
            "Epoch 1, Batch 400, Loss: 0.36909933358430863\n",
            "Epoch 1, Batch 500, Loss: 0.32774952441453936\n",
            "Epoch 1, Batch 600, Loss: 0.29199860885739326\n",
            "Epoch 1, Batch 700, Loss: 0.25970675244927405\n",
            "Epoch 1, Batch 800, Loss: 0.24575468681752682\n",
            "Epoch 1, Batch 900, Loss: 0.2565399082750082\n",
            "Epoch 1, Accuracy on Training dataset: 0.8769166666666667\n",
            "Epoch 2, Batch 100, Loss: 0.20534313034266233\n",
            "Epoch 2, Batch 200, Loss: 0.20424206618219615\n",
            "Epoch 2, Batch 300, Loss: 0.196120393499732\n",
            "Epoch 2, Batch 400, Loss: 0.17911764908581973\n",
            "Epoch 2, Batch 500, Loss: 0.1814156737178564\n",
            "Epoch 2, Batch 600, Loss: 0.16747272823005915\n",
            "Epoch 2, Batch 700, Loss: 0.16155398117378353\n",
            "Epoch 2, Batch 800, Loss: 0.16729812886565923\n",
            "Epoch 2, Batch 900, Loss: 0.16612053219228984\n",
            "Epoch 2, Accuracy on Training dataset: 0.9445833333333333\n",
            "Epoch 3, Batch 100, Loss: 0.13589018022641539\n",
            "Epoch 3, Batch 200, Loss: 0.13307875562459232\n",
            "Epoch 3, Batch 300, Loss: 0.13542838605120777\n",
            "Epoch 3, Batch 400, Loss: 0.135912019982934\n",
            "Epoch 3, Batch 500, Loss: 0.12751160439103842\n",
            "Epoch 3, Batch 600, Loss: 0.12143955456092953\n",
            "Epoch 3, Batch 700, Loss: 0.14148989968001843\n",
            "Epoch 3, Batch 800, Loss: 0.12452519515529276\n",
            "Epoch 3, Batch 900, Loss: 0.13444398703053595\n",
            "Epoch 3, Accuracy on Training dataset: 0.9582\n",
            "Epoch 4, Batch 100, Loss: 0.10385245918296278\n",
            "Epoch 4, Batch 200, Loss: 0.11342127768322825\n",
            "Epoch 4, Batch 300, Loss: 0.11121190981939436\n",
            "Epoch 4, Batch 400, Loss: 0.09984360408037901\n",
            "Epoch 4, Batch 500, Loss: 0.10894310666248203\n",
            "Epoch 4, Batch 600, Loss: 0.10826578775420785\n",
            "Epoch 4, Batch 700, Loss: 0.1088773318938911\n",
            "Epoch 4, Batch 800, Loss: 0.10585950506851077\n",
            "Epoch 4, Batch 900, Loss: 0.10574529179371893\n",
            "Epoch 4, Accuracy on Training dataset: 0.96675\n",
            "Epoch 5, Batch 100, Loss: 0.09934495458379387\n",
            "Epoch 5, Batch 200, Loss: 0.09477074809372425\n",
            "Epoch 5, Batch 300, Loss: 0.08884947429411114\n",
            "Epoch 5, Batch 400, Loss: 0.0959597026091069\n",
            "Epoch 5, Batch 500, Loss: 0.08902483445592224\n",
            "Epoch 5, Batch 600, Loss: 0.09011461203917862\n",
            "Epoch 5, Batch 700, Loss: 0.10433649169281126\n",
            "Epoch 5, Batch 800, Loss: 0.09475841728504747\n",
            "Epoch 5, Batch 900, Loss: 0.08759725955780595\n",
            "Epoch 5, Accuracy on Training dataset: 0.9701833333333333\n",
            "Finished Training\n",
            "Accuracy on testing dataset (modified model - 1): 0.9658666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The modification of adding an extra dense layer of 128 nodes to the original MLP model has resulted in improved performance. After training for 5 epochs, the modified model achieved an impressive training accuracy of approximately 97.02%. The additional layer allows for more complex representations to be learned, enhancing the model's ability to capture intricate patterns in the data. Consequently, the testing accuracy also saw an increase, reaching around 96.59%, indicating that the model's generalization capacity has been enhanced. S, the modification has led to better performance on both the training and testing datasets, demonstrating the effectiveness of the enhancement."
      ],
      "metadata": {
        "id": "xY0xwLVsMotj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Increase the current number of nodes in the layer to 256**"
      ],
      "metadata": {
        "id": "k_AmP1zKXNNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModifiedMLP2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModifiedMLP2, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)  #Increased nodes in the layer to 256\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "#Initializing the modified neural network model 2\n",
        "modified_model2 = ModifiedMLP2()\n",
        "\n",
        "#Defining the loss function and optimizer for modified model 2\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(modified_model2.parameters(), lr=0.001)\n",
        "\n",
        "#Training the modified neural network model 2\n",
        "for epoch in range(num_epochs):\n",
        "    modified_model2.train()\n",
        "    running_loss = 0.0\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    for i, data in enumerate(QMNIST_train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = modified_model2(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Calculate train accuracy\n",
        "    train_accuracy = correct_train / total_train\n",
        "    print(f'Epoch {epoch + 1}, Accuracy on Training dataset: {train_accuracy}')\n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "#Evaluating the modified model 2 on test set\n",
        "modified_model2.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in QMNIST_test_loader:\n",
        "        images, labels = data\n",
        "        outputs = modified_model2(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "modified_test_accuracy2 = correct / total\n",
        "print(f'Accuracy on testing dataset (modified model - 2): {modified_test_accuracy2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrjxG42fXQSQ",
        "outputId": "8c042870-2d2f-4a4e-d830-ce773558a602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 0.9010415947437287\n",
            "Epoch 1, Batch 200, Loss: 0.3955633771419525\n",
            "Epoch 1, Batch 300, Loss: 0.36391509786248205\n",
            "Epoch 1, Batch 400, Loss: 0.319299680814147\n",
            "Epoch 1, Batch 500, Loss: 0.297669121325016\n",
            "Epoch 1, Batch 600, Loss: 0.24732958257198334\n",
            "Epoch 1, Batch 700, Loss: 0.2510384574532509\n",
            "Epoch 1, Batch 800, Loss: 0.2356664188206196\n",
            "Epoch 1, Batch 900, Loss: 0.22380957923829556\n",
            "Epoch 1, Accuracy on Training dataset: 0.8940666666666667\n",
            "Epoch 2, Batch 100, Loss: 0.1768389517441392\n",
            "Epoch 2, Batch 200, Loss: 0.18253634180873632\n",
            "Epoch 2, Batch 300, Loss: 0.17071201883256434\n",
            "Epoch 2, Batch 400, Loss: 0.16457284953445195\n",
            "Epoch 2, Batch 500, Loss: 0.15170896846801044\n",
            "Epoch 2, Batch 600, Loss: 0.1544948105700314\n",
            "Epoch 2, Batch 700, Loss: 0.14764259301126004\n",
            "Epoch 2, Batch 800, Loss: 0.14175523953512312\n",
            "Epoch 2, Batch 900, Loss: 0.15761282008141278\n",
            "Epoch 2, Accuracy on Training dataset: 0.9509\n",
            "Epoch 3, Batch 100, Loss: 0.10786966759711504\n",
            "Epoch 3, Batch 200, Loss: 0.11500366132706404\n",
            "Epoch 3, Batch 300, Loss: 0.11718663373962045\n",
            "Epoch 3, Batch 400, Loss: 0.10914460899308324\n",
            "Epoch 3, Batch 500, Loss: 0.1121699823345989\n",
            "Epoch 3, Batch 600, Loss: 0.12256988156586886\n",
            "Epoch 3, Batch 700, Loss: 0.12151912518776953\n",
            "Epoch 3, Batch 800, Loss: 0.1302520091086626\n",
            "Epoch 3, Batch 900, Loss: 0.10326910794712603\n",
            "Epoch 3, Accuracy on Training dataset: 0.9636833333333333\n",
            "Epoch 4, Batch 100, Loss: 0.09262652870267629\n",
            "Epoch 4, Batch 200, Loss: 0.08933008867315947\n",
            "Epoch 4, Batch 300, Loss: 0.08815831734798849\n",
            "Epoch 4, Batch 400, Loss: 0.09491607810370624\n",
            "Epoch 4, Batch 500, Loss: 0.0966147380322218\n",
            "Epoch 4, Batch 600, Loss: 0.08807192192412913\n",
            "Epoch 4, Batch 700, Loss: 0.0983557192562148\n",
            "Epoch 4, Batch 800, Loss: 0.08741888520307839\n",
            "Epoch 4, Batch 900, Loss: 0.09971317364834249\n",
            "Epoch 4, Accuracy on Training dataset: 0.9708166666666667\n",
            "Epoch 5, Batch 100, Loss: 0.07372370027936995\n",
            "Epoch 5, Batch 200, Loss: 0.08012302372604609\n",
            "Epoch 5, Batch 300, Loss: 0.08496482963208109\n",
            "Epoch 5, Batch 400, Loss: 0.07355182208586485\n",
            "Epoch 5, Batch 500, Loss: 0.07226344487629831\n",
            "Epoch 5, Batch 600, Loss: 0.08361570586450398\n",
            "Epoch 5, Batch 700, Loss: 0.08367009149398655\n",
            "Epoch 5, Batch 800, Loss: 0.08081557461526245\n",
            "Epoch 5, Batch 900, Loss: 0.0830043512256816\n",
            "Epoch 5, Accuracy on Training dataset: 0.9740833333333333\n",
            "Finished Training\n",
            "Accuracy on testing dataset (modified model - 2): 0.9665833333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the hidden layer's node count to 256 in the adjusted MLP model has shown promising results. After training for 5 epochs, the model achieved a training accuracy of about 97.41%, indicating its ability to capture complex patterns in the data due to the increased complexity offered by the larger layer. As a result, testing accuracy also improved, reaching approximately 96.66%, demonstrating an enhanced generalization capacity for the model. Overall, this modification has resulted in improved performance on both training and testing datasets, underscoring the effectiveness of increasing the layer's node count."
      ],
      "metadata": {
        "id": "c4JBLggtNABz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Step 7. Experiment with different optimizers, loss functions, dropout, and activation functions, and observe the change in performance as you tune these hyperparameters.**"
      ],
      "metadata": {
        "id": "q6qSZf5NYoFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Hypothesis-1**\n",
        "\n",
        " * 3 fully connected layers with LogSoftmax activation function between them.\n",
        "\n",
        " * Dropout rate of 0.3 applied after the first fully connected layer\n",
        "\n",
        " * Negative Log Likelihood Loss (NLLLoss) used as the loss function\n",
        "\n",
        " * Stochastic Gradient Descent (SGD) optimizer with epochs as 10"
      ],
      "metadata": {
        "id": "ACQHi8tqZgw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture hypothesis 1\n",
        "class Hypothesis1Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypothesis1Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize the hypothesis hypothesis 1\n",
        "model = Hypothesis1Model()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Train the hypothesis_1 model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    for i, data in enumerate(QMNIST_train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted_train = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Calculate train accuracy\n",
        "    train_accuracy = correct_train / total_train\n",
        "    print(f'Epoch {epoch + 1}, Accuracy on Training dataset: {train_accuracy}')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluate the hypothesis 1 model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in QMNIST_test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on testing dataset: {correct / total}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afqsxu_HsVqb",
        "outputId": "ee8f54e4-a782-4ca0-a2f3-a2a283797ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 2.2964222383499147\n",
            "Epoch 1, Batch 200, Loss: 2.282272663116455\n",
            "Epoch 1, Batch 300, Loss: 2.2665941452980043\n",
            "Epoch 1, Batch 400, Loss: 2.257044305801392\n",
            "Epoch 1, Batch 500, Loss: 2.243859798908234\n",
            "Epoch 1, Batch 600, Loss: 2.232000586986542\n",
            "Epoch 1, Batch 700, Loss: 2.21800567150116\n",
            "Epoch 1, Batch 800, Loss: 2.2060617351531984\n",
            "Epoch 1, Batch 900, Loss: 2.191219172477722\n",
            "Epoch 1, Accuracy on Training dataset: 0.23978333333333332\n",
            "Epoch 2, Batch 100, Loss: 2.165939347743988\n",
            "Epoch 2, Batch 200, Loss: 2.1403634357452392\n",
            "Epoch 2, Batch 300, Loss: 2.1234990620613097\n",
            "Epoch 2, Batch 400, Loss: 2.099569869041443\n",
            "Epoch 2, Batch 500, Loss: 2.074424855709076\n",
            "Epoch 2, Batch 600, Loss: 2.0487805247306823\n",
            "Epoch 2, Batch 700, Loss: 2.0220170056819917\n",
            "Epoch 2, Batch 800, Loss: 1.9842404901981354\n",
            "Epoch 2, Batch 900, Loss: 1.94229061126709\n",
            "Epoch 2, Accuracy on Training dataset: 0.48275\n",
            "Epoch 3, Batch 100, Loss: 1.9023265469074249\n",
            "Epoch 3, Batch 200, Loss: 1.8467917919158936\n",
            "Epoch 3, Batch 300, Loss: 1.811849535703659\n",
            "Epoch 3, Batch 400, Loss: 1.7535782849788666\n",
            "Epoch 3, Batch 500, Loss: 1.716315141916275\n",
            "Epoch 3, Batch 600, Loss: 1.6674135756492614\n",
            "Epoch 3, Batch 700, Loss: 1.618099755048752\n",
            "Epoch 3, Batch 800, Loss: 1.5644664824008943\n",
            "Epoch 3, Batch 900, Loss: 1.509038826227188\n",
            "Epoch 3, Accuracy on Training dataset: 0.5863666666666667\n",
            "Epoch 4, Batch 100, Loss: 1.4502659261226654\n",
            "Epoch 4, Batch 200, Loss: 1.3995297837257386\n",
            "Epoch 4, Batch 300, Loss: 1.369711881875992\n",
            "Epoch 4, Batch 400, Loss: 1.3229371297359467\n",
            "Epoch 4, Batch 500, Loss: 1.2738719546794892\n",
            "Epoch 4, Batch 600, Loss: 1.2412797248363494\n",
            "Epoch 4, Batch 700, Loss: 1.2085627329349518\n",
            "Epoch 4, Batch 800, Loss: 1.1518321484327316\n",
            "Epoch 4, Batch 900, Loss: 1.111642714738846\n",
            "Epoch 4, Accuracy on Training dataset: 0.67255\n",
            "Epoch 5, Batch 100, Loss: 1.0894504636526108\n",
            "Epoch 5, Batch 200, Loss: 1.0657548308372498\n",
            "Epoch 5, Batch 300, Loss: 1.0238968378305435\n",
            "Epoch 5, Batch 400, Loss: 1.0103018170595168\n",
            "Epoch 5, Batch 500, Loss: 0.9807241666316986\n",
            "Epoch 5, Batch 600, Loss: 0.9519038522243499\n",
            "Epoch 5, Batch 700, Loss: 0.9451029181480408\n",
            "Epoch 5, Batch 800, Loss: 0.921498516201973\n",
            "Epoch 5, Batch 900, Loss: 0.909316321015358\n",
            "Epoch 5, Accuracy on Training dataset: 0.7326166666666667\n",
            "Epoch 6, Batch 100, Loss: 0.8840644311904907\n",
            "Epoch 6, Batch 200, Loss: 0.837108970284462\n",
            "Epoch 6, Batch 300, Loss: 0.8610715544223786\n",
            "Epoch 6, Batch 400, Loss: 0.8523241710662842\n",
            "Epoch 6, Batch 500, Loss: 0.8075357300043106\n",
            "Epoch 6, Batch 600, Loss: 0.8143848109245301\n",
            "Epoch 6, Batch 700, Loss: 0.7812369900941849\n",
            "Epoch 6, Batch 800, Loss: 0.7881277877092362\n",
            "Epoch 6, Batch 900, Loss: 0.7772438937425613\n",
            "Epoch 6, Accuracy on Training dataset: 0.7692\n",
            "Epoch 7, Batch 100, Loss: 0.7487343448400497\n",
            "Epoch 7, Batch 200, Loss: 0.754360386133194\n",
            "Epoch 7, Batch 300, Loss: 0.7378224182128906\n",
            "Epoch 7, Batch 400, Loss: 0.7212374743819236\n",
            "Epoch 7, Batch 500, Loss: 0.7248907625675202\n",
            "Epoch 7, Batch 600, Loss: 0.7027641558647155\n",
            "Epoch 7, Batch 700, Loss: 0.6953342178463936\n",
            "Epoch 7, Batch 800, Loss: 0.712836686372757\n",
            "Epoch 7, Batch 900, Loss: 0.6831957197189331\n",
            "Epoch 7, Accuracy on Training dataset: 0.7921833333333334\n",
            "Epoch 8, Batch 100, Loss: 0.6671953257918358\n",
            "Epoch 8, Batch 200, Loss: 0.6595997616648674\n",
            "Epoch 8, Batch 300, Loss: 0.6641230815649033\n",
            "Epoch 8, Batch 400, Loss: 0.6649341604113579\n",
            "Epoch 8, Batch 500, Loss: 0.6700342458486557\n",
            "Epoch 8, Batch 600, Loss: 0.6358589231967926\n",
            "Epoch 8, Batch 700, Loss: 0.6366238075494767\n",
            "Epoch 8, Batch 800, Loss: 0.616315695643425\n",
            "Epoch 8, Batch 900, Loss: 0.6297120839357376\n",
            "Epoch 8, Accuracy on Training dataset: 0.8113666666666667\n",
            "Epoch 9, Batch 100, Loss: 0.6292281469702721\n",
            "Epoch 9, Batch 200, Loss: 0.6232892674207687\n",
            "Epoch 9, Batch 300, Loss: 0.606936063170433\n",
            "Epoch 9, Batch 400, Loss: 0.6182977277040481\n",
            "Epoch 9, Batch 500, Loss: 0.594000823199749\n",
            "Epoch 9, Batch 600, Loss: 0.5904167291522026\n",
            "Epoch 9, Batch 700, Loss: 0.5878116318583488\n",
            "Epoch 9, Batch 800, Loss: 0.5746762889623642\n",
            "Epoch 9, Batch 900, Loss: 0.5818283221125603\n",
            "Epoch 9, Accuracy on Training dataset: 0.8245\n",
            "Epoch 10, Batch 100, Loss: 0.5880693614482879\n",
            "Epoch 10, Batch 200, Loss: 0.5810570392012596\n",
            "Epoch 10, Batch 300, Loss: 0.5565925857424736\n",
            "Epoch 10, Batch 400, Loss: 0.5811963069438935\n",
            "Epoch 10, Batch 500, Loss: 0.5554416209459305\n",
            "Epoch 10, Batch 600, Loss: 0.5557063719630242\n",
            "Epoch 10, Batch 700, Loss: 0.5648998856544495\n",
            "Epoch 10, Batch 800, Loss: 0.5617096155881882\n",
            "Epoch 10, Batch 900, Loss: 0.5325152945518493\n",
            "Epoch 10, Accuracy on Training dataset: 0.8347333333333333\n",
            "Finished Training\n",
            "Accuracy on testing dataset: 0.8739333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first hypothesis model includes three connected layers with LogSoftmax activation functions. A dropout rate of 0.3 is used after the initial fully connected layer to avoid overfitting. The NLLLoss function acts as the loss function, while SGD is utilized as the optimizer with a learning rate of 0.001. Training for 10 epochs was carried out on this model.\n",
        "\n",
        "A lower learning rate tends to result in slower but more stable training. With a learning rate of 0.001, the model's loss gradually decreases over epochs, indicating a smooth convergence.\n",
        "The choice of optimizer is impacting how the model updates its parameters during training.\n",
        "\n",
        "The model reached a training accuracy of about 83.47% after the 10th epoch, and the loss decreased consistently as the training went on, showing that the model was effectively learning from the dataset. Upon evaluation on the testing dataset, the model achieved an accuracy of about 87.39%, demonstrating its ability to generalize well to unseen data."
      ],
      "metadata": {
        "id": "uuFP5JSPNXXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Hypothesis-2**\n",
        "\n",
        " * 4 fully connected layers with sigmoid activation function between them\n",
        " * Dropout rate of 0.7 applied after the first and third fully connected layers\n",
        " * Binary Cross Entropy with Logits Loss (BCEWithLogitsLoss) used as the loss function and Adagrad as Optimizer\n",
        " * Adagrad optimizer with learning rate as 0.01 and with 15 epochs"
      ],
      "metadata": {
        "id": "6KoVSzQTusvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture hypothesis model 2\n",
        "class Hypothesis2Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypothesis2Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the hypothesis model 2\n",
        "hypothesis_model_2 = Hypothesis2Model()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adagrad(hypothesis_model_2.parameters(), lr=0.01)\n",
        "\n",
        "# Train the hypothesis model 2\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    hypothesis_model_2.train()\n",
        "    running_loss = 0.0\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    for i, data in enumerate(QMNIST_train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = hypothesis_model_2(inputs)\n",
        "\n",
        "        # One-hot encode the labels\n",
        "        labels_onehot = torch.zeros_like(outputs)\n",
        "        labels_onehot.scatter_(1, labels.view(-1, 1), 1.0)\n",
        "\n",
        "        loss = criterion(outputs, labels_onehot)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        predicted_train = outputs > 0.5\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels_onehot).sum().item()\n",
        "\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Calculate train accuracy\n",
        "    train_accuracy = correct_train / total_train\n",
        "    print(f'Epoch {epoch + 1}, Accuracy on Training dataset: {train_accuracy}')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluate the hypothesis model 2\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in QMNIST_test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on testing dataset: {correct / total}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1LjQjmBuws0",
        "outputId": "411ee2f0-1297-4a00-b1f1-31306a7f18bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 0.36641561925411226\n",
            "Epoch 1, Batch 200, Loss: 0.34051380068063736\n",
            "Epoch 1, Batch 300, Loss: 0.3176856505870819\n",
            "Epoch 1, Batch 400, Loss: 0.29351087152957916\n",
            "Epoch 1, Batch 500, Loss: 0.2736477580666542\n",
            "Epoch 1, Batch 600, Loss: 0.2595393231511116\n",
            "Epoch 1, Batch 700, Loss: 0.24746610566973687\n",
            "Epoch 1, Batch 800, Loss: 0.24029458537697793\n",
            "Epoch 1, Batch 900, Loss: 0.22627809807658195\n",
            "Epoch 1, Accuracy on Training dataset: 9.004166666666666\n",
            "Epoch 2, Batch 100, Loss: 0.21672579184174537\n",
            "Epoch 2, Batch 200, Loss: 0.20815501004457473\n",
            "Epoch 2, Batch 300, Loss: 0.2009918373823166\n",
            "Epoch 2, Batch 400, Loss: 0.19611442640423773\n",
            "Epoch 2, Batch 500, Loss: 0.19028647765517234\n",
            "Epoch 2, Batch 600, Loss: 0.18718621924519538\n",
            "Epoch 2, Batch 700, Loss: 0.1828821773827076\n",
            "Epoch 2, Batch 800, Loss: 0.17903639540076255\n",
            "Epoch 2, Batch 900, Loss: 0.17738087669014932\n",
            "Epoch 2, Accuracy on Training dataset: 9.116666666666667\n",
            "Epoch 3, Batch 100, Loss: 0.17292431384325027\n",
            "Epoch 3, Batch 200, Loss: 0.16853491470217705\n",
            "Epoch 3, Batch 300, Loss: 0.16602503195405005\n",
            "Epoch 3, Batch 400, Loss: 0.16534535855054855\n",
            "Epoch 3, Batch 500, Loss: 0.1624054081737995\n",
            "Epoch 3, Batch 600, Loss: 0.16177077502012252\n",
            "Epoch 3, Batch 700, Loss: 0.15863022714853287\n",
            "Epoch 3, Batch 800, Loss: 0.15658005326986313\n",
            "Epoch 3, Batch 900, Loss: 0.15264789029955864\n",
            "Epoch 3, Accuracy on Training dataset: 9.2558\n",
            "Epoch 4, Batch 100, Loss: 0.15244115352630616\n",
            "Epoch 4, Batch 200, Loss: 0.15172374300658703\n",
            "Epoch 4, Batch 300, Loss: 0.14920432947576046\n",
            "Epoch 4, Batch 400, Loss: 0.14621552057564258\n",
            "Epoch 4, Batch 500, Loss: 0.1442675030231476\n",
            "Epoch 4, Batch 600, Loss: 0.147801251411438\n",
            "Epoch 4, Batch 700, Loss: 0.14475894555449487\n",
            "Epoch 4, Batch 800, Loss: 0.1416459984332323\n",
            "Epoch 4, Batch 900, Loss: 0.1427639961987734\n",
            "Epoch 4, Accuracy on Training dataset: 9.346566666666666\n",
            "Epoch 5, Batch 100, Loss: 0.13785386934876442\n",
            "Epoch 5, Batch 200, Loss: 0.13991822861135006\n",
            "Epoch 5, Batch 300, Loss: 0.13711473368108273\n",
            "Epoch 5, Batch 400, Loss: 0.13825371235609055\n",
            "Epoch 5, Batch 500, Loss: 0.13649476796388627\n",
            "Epoch 5, Batch 600, Loss: 0.13365676745772362\n",
            "Epoch 5, Batch 700, Loss: 0.12850681968033315\n",
            "Epoch 5, Batch 800, Loss: 0.1339385963231325\n",
            "Epoch 5, Batch 900, Loss: 0.12732058681547642\n",
            "Epoch 5, Accuracy on Training dataset: 9.411083333333334\n",
            "Epoch 6, Batch 100, Loss: 0.13099968560039998\n",
            "Epoch 6, Batch 200, Loss: 0.13108044862747192\n",
            "Epoch 6, Batch 300, Loss: 0.12924083769321443\n",
            "Epoch 6, Batch 400, Loss: 0.12709071375429631\n",
            "Epoch 6, Batch 500, Loss: 0.12800209887325764\n",
            "Epoch 6, Batch 600, Loss: 0.12576829664409162\n",
            "Epoch 6, Batch 700, Loss: 0.1252582011371851\n",
            "Epoch 6, Batch 800, Loss: 0.12379551902413369\n",
            "Epoch 6, Batch 900, Loss: 0.12262174934148788\n",
            "Epoch 6, Accuracy on Training dataset: 9.466083333333334\n",
            "Epoch 7, Batch 100, Loss: 0.12353136233985423\n",
            "Epoch 7, Batch 200, Loss: 0.12319516144692898\n",
            "Epoch 7, Batch 300, Loss: 0.12099083237349988\n",
            "Epoch 7, Batch 400, Loss: 0.12154639810323715\n",
            "Epoch 7, Batch 500, Loss: 0.12094744019210339\n",
            "Epoch 7, Batch 600, Loss: 0.12003198511898518\n",
            "Epoch 7, Batch 700, Loss: 0.11810347184538841\n",
            "Epoch 7, Batch 800, Loss: 0.1187451159954071\n",
            "Epoch 7, Batch 900, Loss: 0.11711509972810745\n",
            "Epoch 7, Accuracy on Training dataset: 9.5111\n",
            "Epoch 8, Batch 100, Loss: 0.11665729247033596\n",
            "Epoch 8, Batch 200, Loss: 0.11964551120996475\n",
            "Epoch 8, Batch 300, Loss: 0.11400983341038227\n",
            "Epoch 8, Batch 400, Loss: 0.11683268800377845\n",
            "Epoch 8, Batch 500, Loss: 0.1134996161609888\n",
            "Epoch 8, Batch 600, Loss: 0.11750177286565304\n",
            "Epoch 8, Batch 700, Loss: 0.1117151878774166\n",
            "Epoch 8, Batch 800, Loss: 0.11531642250716687\n",
            "Epoch 8, Batch 900, Loss: 0.11442421056330204\n",
            "Epoch 8, Accuracy on Training dataset: 9.54765\n",
            "Epoch 9, Batch 100, Loss: 0.11189794220030308\n",
            "Epoch 9, Batch 200, Loss: 0.11375653602182866\n",
            "Epoch 9, Batch 300, Loss: 0.1111341093480587\n",
            "Epoch 9, Batch 400, Loss: 0.10890555478632451\n",
            "Epoch 9, Batch 500, Loss: 0.11108135916292668\n",
            "Epoch 9, Batch 600, Loss: 0.11199500016868115\n",
            "Epoch 9, Batch 700, Loss: 0.11052592411637306\n",
            "Epoch 9, Batch 800, Loss: 0.11162645988166332\n",
            "Epoch 9, Batch 900, Loss: 0.10499086789786816\n",
            "Epoch 9, Accuracy on Training dataset: 9.580366666666666\n",
            "Epoch 10, Batch 100, Loss: 0.10937572225928306\n",
            "Epoch 10, Batch 200, Loss: 0.1083423686772585\n",
            "Epoch 10, Batch 300, Loss: 0.10956438139081001\n",
            "Epoch 10, Batch 400, Loss: 0.10726915940642356\n",
            "Epoch 10, Batch 500, Loss: 0.10410884283483028\n",
            "Epoch 10, Batch 600, Loss: 0.10843416012823581\n",
            "Epoch 10, Batch 700, Loss: 0.11091194726526737\n",
            "Epoch 10, Batch 800, Loss: 0.10440079063177109\n",
            "Epoch 10, Batch 900, Loss: 0.10677371174097061\n",
            "Epoch 10, Accuracy on Training dataset: 9.598616666666667\n",
            "Epoch 11, Batch 100, Loss: 0.10473034240305423\n",
            "Epoch 11, Batch 200, Loss: 0.105723856985569\n",
            "Epoch 11, Batch 300, Loss: 0.1070128732174635\n",
            "Epoch 11, Batch 400, Loss: 0.10659261666238308\n",
            "Epoch 11, Batch 500, Loss: 0.10231381669640541\n",
            "Epoch 11, Batch 600, Loss: 0.10412120074033737\n",
            "Epoch 11, Batch 700, Loss: 0.10571840539574623\n",
            "Epoch 11, Batch 800, Loss: 0.10311748929321767\n",
            "Epoch 11, Batch 900, Loss: 0.10228664815425872\n",
            "Epoch 11, Accuracy on Training dataset: 9.616916666666667\n",
            "Epoch 12, Batch 100, Loss: 0.10153127439320088\n",
            "Epoch 12, Batch 200, Loss: 0.10290525622665882\n",
            "Epoch 12, Batch 300, Loss: 0.10230790629982948\n",
            "Epoch 12, Batch 400, Loss: 0.101946871727705\n",
            "Epoch 12, Batch 500, Loss: 0.10398889727890491\n",
            "Epoch 12, Batch 600, Loss: 0.10125260666012764\n",
            "Epoch 12, Batch 700, Loss: 0.10070888839662075\n",
            "Epoch 12, Batch 800, Loss: 0.10589913949370384\n",
            "Epoch 12, Batch 900, Loss: 0.1016218701004982\n",
            "Epoch 12, Accuracy on Training dataset: 9.630216666666668\n",
            "Epoch 13, Batch 100, Loss: 0.10417512580752372\n",
            "Epoch 13, Batch 200, Loss: 0.09988631881773471\n",
            "Epoch 13, Batch 300, Loss: 0.1005955557897687\n",
            "Epoch 13, Batch 400, Loss: 0.10214592278003692\n",
            "Epoch 13, Batch 500, Loss: 0.09771284885704518\n",
            "Epoch 13, Batch 600, Loss: 0.10106879204511643\n",
            "Epoch 13, Batch 700, Loss: 0.09834126174449921\n",
            "Epoch 13, Batch 800, Loss: 0.09754716135561466\n",
            "Epoch 13, Batch 900, Loss: 0.09559492886066437\n",
            "Epoch 13, Accuracy on Training dataset: 9.64525\n",
            "Epoch 14, Batch 100, Loss: 0.09701723616570235\n",
            "Epoch 14, Batch 200, Loss: 0.10039470240473747\n",
            "Epoch 14, Batch 300, Loss: 0.09736333101987839\n",
            "Epoch 14, Batch 400, Loss: 0.0995723082125187\n",
            "Epoch 14, Batch 500, Loss: 0.09573196705430746\n",
            "Epoch 14, Batch 600, Loss: 0.09989724934101105\n",
            "Epoch 14, Batch 700, Loss: 0.09684047780930996\n",
            "Epoch 14, Batch 800, Loss: 0.09605465590953827\n",
            "Epoch 14, Batch 900, Loss: 0.09803552135825157\n",
            "Epoch 14, Accuracy on Training dataset: 9.656333333333333\n",
            "Epoch 15, Batch 100, Loss: 0.09595098048448562\n",
            "Epoch 15, Batch 200, Loss: 0.09584075137972832\n",
            "Epoch 15, Batch 300, Loss: 0.09153242520987988\n",
            "Epoch 15, Batch 400, Loss: 0.09555543802678584\n",
            "Epoch 15, Batch 500, Loss: 0.09877672232687473\n",
            "Epoch 15, Batch 600, Loss: 0.09414329506456852\n",
            "Epoch 15, Batch 700, Loss: 0.09658472817391157\n",
            "Epoch 15, Batch 800, Loss: 0.0961287496611476\n",
            "Epoch 15, Batch 900, Loss: 0.0946074002981186\n",
            "Epoch 15, Accuracy on Training dataset: 9.6676\n",
            "Finished Training\n",
            "Accuracy on testing dataset: 0.8739333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second hypothesis model consists of four fully linked layers with sigmoid activation functions. Binary Cross Entropy with Logits Loss is used as the loss function, and Adagrad as optimizer with a learning rate of 0.01 for training the model for 15 epochs with dropout rate as 0.7.\n",
        "\n",
        "During the training, the model shows an unusual pattern where the training accuracy stays consistently low at about 9.65%. This indicates that the model might not be effectively learning from the training data. The dropout rate of 0.7 is meant to prevent overfitting but it's too high and seems to be preventing the model from learning meaningful patterns from the data during training. The loss values seem quite high, suggesting that the model is having a tough time converging. When we look at the test dataset, the model achieves an accuracy of approximately 87.39%, matching hypothesis-1's accuracy."
      ],
      "metadata": {
        "id": "PXtEVY-nN2Hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Hypothesis-3**\n",
        "\n",
        " * Model architecture is more complex with 4 fully connected layers and more neurons\n",
        "\n",
        " * Dropout rate of 0.5 is applied after the first fully connected layer\n",
        "\n",
        " * Adam optimizer is used with a learning rate of 0.0000001 and epochs of 20\n",
        "\n",
        " * reLU as activation function and crossentropy as loss function"
      ],
      "metadata": {
        "id": "kxl1bhk9z1sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture hypothesis model 3\n",
        "class Hypothesis3Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypothesis3Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the hypothesis model 3\n",
        "hypothesis_model_3 = Hypothesis3Model()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(hypothesis_model_3.parameters(), lr=0.0000001)\n",
        "\n",
        "# Train the hypothesis model 3\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    hypothesis_model_3.train()\n",
        "    running_loss = 0.0\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    for i, data in enumerate(QMNIST_train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = hypothesis_model_3(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted_train = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Calculate train accuracy\n",
        "    train_accuracy = correct_train / total_train\n",
        "    print(f'Epoch {epoch + 1}, Accuracy on Training dataset: {train_accuracy}')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluate the hypothesis model 3\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in QMNIST_test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on testing dataset: {correct / total}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI3dys74z8fr",
        "outputId": "6d21c02b-c585-4f1f-f050-5b79a42a21fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 2.3047374367713926\n",
            "Epoch 1, Batch 200, Loss: 2.3050670051574706\n",
            "Epoch 1, Batch 300, Loss: 2.305504801273346\n",
            "Epoch 1, Batch 400, Loss: 2.3029680943489073\n",
            "Epoch 1, Batch 500, Loss: 2.3050238037109376\n",
            "Epoch 1, Batch 600, Loss: 2.3046005058288572\n",
            "Epoch 1, Batch 700, Loss: 2.305506777763367\n",
            "Epoch 1, Batch 800, Loss: 2.3045759534835817\n",
            "Epoch 1, Batch 900, Loss: 2.3043960237503054\n",
            "Epoch 1, Accuracy on Training dataset: 0.0915\n",
            "Epoch 2, Batch 100, Loss: 2.3044574570655825\n",
            "Epoch 2, Batch 200, Loss: 2.3034186005592345\n",
            "Epoch 2, Batch 300, Loss: 2.3053583884239197\n",
            "Epoch 2, Batch 400, Loss: 2.304675464630127\n",
            "Epoch 2, Batch 500, Loss: 2.304005265235901\n",
            "Epoch 2, Batch 600, Loss: 2.3041434144973754\n",
            "Epoch 2, Batch 700, Loss: 2.304326207637787\n",
            "Epoch 2, Batch 800, Loss: 2.3047228980064394\n",
            "Epoch 2, Batch 900, Loss: 2.303987929821014\n",
            "Epoch 2, Accuracy on Training dataset: 0.09233333333333334\n",
            "Epoch 3, Batch 100, Loss: 2.303482940196991\n",
            "Epoch 3, Batch 200, Loss: 2.305009686946869\n",
            "Epoch 3, Batch 300, Loss: 2.3040247631072996\n",
            "Epoch 3, Batch 400, Loss: 2.3041755819320677\n",
            "Epoch 3, Batch 500, Loss: 2.3039942932128907\n",
            "Epoch 3, Batch 600, Loss: 2.3019886660575866\n",
            "Epoch 3, Batch 700, Loss: 2.3031827640533447\n",
            "Epoch 3, Batch 800, Loss: 2.3043953204154968\n",
            "Epoch 3, Batch 900, Loss: 2.3034077191352846\n",
            "Epoch 3, Accuracy on Training dataset: 0.09256666666666667\n",
            "Epoch 4, Batch 100, Loss: 2.3051339983940125\n",
            "Epoch 4, Batch 200, Loss: 2.3037195086479185\n",
            "Epoch 4, Batch 300, Loss: 2.302384216785431\n",
            "Epoch 4, Batch 400, Loss: 2.3050385189056395\n",
            "Epoch 4, Batch 500, Loss: 2.3037368178367617\n",
            "Epoch 4, Batch 600, Loss: 2.302111361026764\n",
            "Epoch 4, Batch 700, Loss: 2.303507912158966\n",
            "Epoch 4, Batch 800, Loss: 2.3022714042663575\n",
            "Epoch 4, Batch 900, Loss: 2.3030887341499326\n",
            "Epoch 4, Accuracy on Training dataset: 0.09291666666666666\n",
            "Epoch 5, Batch 100, Loss: 2.303878035545349\n",
            "Epoch 5, Batch 200, Loss: 2.3021982407569883\n",
            "Epoch 5, Batch 300, Loss: 2.303261616230011\n",
            "Epoch 5, Batch 400, Loss: 2.3021366477012633\n",
            "Epoch 5, Batch 500, Loss: 2.3018567633628844\n",
            "Epoch 5, Batch 600, Loss: 2.302414507865906\n",
            "Epoch 5, Batch 700, Loss: 2.3027212762832643\n",
            "Epoch 5, Batch 800, Loss: 2.30285587310791\n",
            "Epoch 5, Batch 900, Loss: 2.3024888968467714\n",
            "Epoch 5, Accuracy on Training dataset: 0.0934\n",
            "Epoch 6, Batch 100, Loss: 2.302962021827698\n",
            "Epoch 6, Batch 200, Loss: 2.3019820713996886\n",
            "Epoch 6, Batch 300, Loss: 2.303473470211029\n",
            "Epoch 6, Batch 400, Loss: 2.3020375680923464\n",
            "Epoch 6, Batch 500, Loss: 2.302192008495331\n",
            "Epoch 6, Batch 600, Loss: 2.3010202074050903\n",
            "Epoch 6, Batch 700, Loss: 2.301599884033203\n",
            "Epoch 6, Batch 800, Loss: 2.3030388617515563\n",
            "Epoch 6, Batch 900, Loss: 2.301675238609314\n",
            "Epoch 6, Accuracy on Training dataset: 0.09533333333333334\n",
            "Epoch 7, Batch 100, Loss: 2.301927156448364\n",
            "Epoch 7, Batch 200, Loss: 2.302553863525391\n",
            "Epoch 7, Batch 300, Loss: 2.3020362401008607\n",
            "Epoch 7, Batch 400, Loss: 2.301768190860748\n",
            "Epoch 7, Batch 500, Loss: 2.3017847037315367\n",
            "Epoch 7, Batch 600, Loss: 2.3018679332733156\n",
            "Epoch 7, Batch 700, Loss: 2.301528723239899\n",
            "Epoch 7, Batch 800, Loss: 2.3018574476242066\n",
            "Epoch 7, Batch 900, Loss: 2.3012704706192015\n",
            "Epoch 7, Accuracy on Training dataset: 0.09505\n",
            "Epoch 8, Batch 100, Loss: 2.30208571434021\n",
            "Epoch 8, Batch 200, Loss: 2.301179814338684\n",
            "Epoch 8, Batch 300, Loss: 2.301604058742523\n",
            "Epoch 8, Batch 400, Loss: 2.3005313730239867\n",
            "Epoch 8, Batch 500, Loss: 2.3009514808654785\n",
            "Epoch 8, Batch 600, Loss: 2.3018366837501527\n",
            "Epoch 8, Batch 700, Loss: 2.2999308037757875\n",
            "Epoch 8, Batch 800, Loss: 2.301188328266144\n",
            "Epoch 8, Batch 900, Loss: 2.3006447768211364\n",
            "Epoch 8, Accuracy on Training dataset: 0.09563333333333333\n",
            "Epoch 9, Batch 100, Loss: 2.300494604110718\n",
            "Epoch 9, Batch 200, Loss: 2.30158586025238\n",
            "Epoch 9, Batch 300, Loss: 2.299486083984375\n",
            "Epoch 9, Batch 400, Loss: 2.3007412219047545\n",
            "Epoch 9, Batch 500, Loss: 2.3009225583076476\n",
            "Epoch 9, Batch 600, Loss: 2.3006860852241515\n",
            "Epoch 9, Batch 700, Loss: 2.300465819835663\n",
            "Epoch 9, Batch 800, Loss: 2.3000529313087466\n",
            "Epoch 9, Batch 900, Loss: 2.3003145813941956\n",
            "Epoch 9, Accuracy on Training dataset: 0.09685\n",
            "Epoch 10, Batch 100, Loss: 2.300733535289764\n",
            "Epoch 10, Batch 200, Loss: 2.300917344093323\n",
            "Epoch 10, Batch 300, Loss: 2.2995538115501404\n",
            "Epoch 10, Batch 400, Loss: 2.2993607449531557\n",
            "Epoch 10, Batch 500, Loss: 2.3003440260887147\n",
            "Epoch 10, Batch 600, Loss: 2.29986478805542\n",
            "Epoch 10, Batch 700, Loss: 2.2993978881835937\n",
            "Epoch 10, Batch 800, Loss: 2.299427714347839\n",
            "Epoch 10, Batch 900, Loss: 2.2989308834075928\n",
            "Epoch 10, Accuracy on Training dataset: 0.10008333333333333\n",
            "Epoch 11, Batch 100, Loss: 2.300073277950287\n",
            "Epoch 11, Batch 200, Loss: 2.2985649347305297\n",
            "Epoch 11, Batch 300, Loss: 2.2987979555130007\n",
            "Epoch 11, Batch 400, Loss: 2.299419808387756\n",
            "Epoch 11, Batch 500, Loss: 2.2996854829788207\n",
            "Epoch 11, Batch 600, Loss: 2.298453884124756\n",
            "Epoch 11, Batch 700, Loss: 2.2999627113342287\n",
            "Epoch 11, Batch 800, Loss: 2.3002003812789917\n",
            "Epoch 11, Batch 900, Loss: 2.3007492518424986\n",
            "Epoch 11, Accuracy on Training dataset: 0.09953333333333333\n",
            "Epoch 12, Batch 100, Loss: 2.300155346393585\n",
            "Epoch 12, Batch 200, Loss: 2.2983955478668214\n",
            "Epoch 12, Batch 300, Loss: 2.2989017581939697\n",
            "Epoch 12, Batch 400, Loss: 2.2984512066841125\n",
            "Epoch 12, Batch 500, Loss: 2.2992743968963625\n",
            "Epoch 12, Batch 600, Loss: 2.2976893305778505\n",
            "Epoch 12, Batch 700, Loss: 2.2990265774726866\n",
            "Epoch 12, Batch 800, Loss: 2.2981993198394775\n",
            "Epoch 12, Batch 900, Loss: 2.298116750717163\n",
            "Epoch 12, Accuracy on Training dataset: 0.10028333333333334\n",
            "Epoch 13, Batch 100, Loss: 2.2975359916687013\n",
            "Epoch 13, Batch 200, Loss: 2.298361120223999\n",
            "Epoch 13, Batch 300, Loss: 2.2984577918052675\n",
            "Epoch 13, Batch 400, Loss: 2.2979956603050233\n",
            "Epoch 13, Batch 500, Loss: 2.2990754103660582\n",
            "Epoch 13, Batch 600, Loss: 2.2984114575386045\n",
            "Epoch 13, Batch 700, Loss: 2.297025878429413\n",
            "Epoch 13, Batch 800, Loss: 2.297710518836975\n",
            "Epoch 13, Batch 900, Loss: 2.2975107288360594\n",
            "Epoch 13, Accuracy on Training dataset: 0.10318333333333334\n",
            "Epoch 14, Batch 100, Loss: 2.2978347969055175\n",
            "Epoch 14, Batch 200, Loss: 2.298420844078064\n",
            "Epoch 14, Batch 300, Loss: 2.2978795075416567\n",
            "Epoch 14, Batch 400, Loss: 2.2971435809135436\n",
            "Epoch 14, Batch 500, Loss: 2.297560815811157\n",
            "Epoch 14, Batch 600, Loss: 2.297857255935669\n",
            "Epoch 14, Batch 700, Loss: 2.2966168880462647\n",
            "Epoch 14, Batch 800, Loss: 2.296856153011322\n",
            "Epoch 14, Batch 900, Loss: 2.2978523755073548\n",
            "Epoch 14, Accuracy on Training dataset: 0.1035\n",
            "Epoch 15, Batch 100, Loss: 2.2968431973457335\n",
            "Epoch 15, Batch 200, Loss: 2.2969001960754394\n",
            "Epoch 15, Batch 300, Loss: 2.2965748643875123\n",
            "Epoch 15, Batch 400, Loss: 2.2964525604248047\n",
            "Epoch 15, Batch 500, Loss: 2.2968864607810975\n",
            "Epoch 15, Batch 600, Loss: 2.297015302181244\n",
            "Epoch 15, Batch 700, Loss: 2.2968587136268614\n",
            "Epoch 15, Batch 800, Loss: 2.296955795288086\n",
            "Epoch 15, Batch 900, Loss: 2.296385202407837\n",
            "Epoch 15, Accuracy on Training dataset: 0.10346666666666667\n",
            "Epoch 16, Batch 100, Loss: 2.295684473514557\n",
            "Epoch 16, Batch 200, Loss: 2.2956993865966795\n",
            "Epoch 16, Batch 300, Loss: 2.295382835865021\n",
            "Epoch 16, Batch 400, Loss: 2.2956060695648195\n",
            "Epoch 16, Batch 500, Loss: 2.2964465117454527\n",
            "Epoch 16, Batch 600, Loss: 2.2958027935028076\n",
            "Epoch 16, Batch 700, Loss: 2.295900721549988\n",
            "Epoch 16, Batch 800, Loss: 2.295484902858734\n",
            "Epoch 16, Batch 900, Loss: 2.2963437676429748\n",
            "Epoch 16, Accuracy on Training dataset: 0.10676666666666666\n",
            "Epoch 17, Batch 100, Loss: 2.294732029438019\n",
            "Epoch 17, Batch 200, Loss: 2.295812466144562\n",
            "Epoch 17, Batch 300, Loss: 2.2960703015327453\n",
            "Epoch 17, Batch 400, Loss: 2.294372329711914\n",
            "Epoch 17, Batch 500, Loss: 2.296453392505646\n",
            "Epoch 17, Batch 600, Loss: 2.29476279258728\n",
            "Epoch 17, Batch 700, Loss: 2.2948671698570253\n",
            "Epoch 17, Batch 800, Loss: 2.295645868778229\n",
            "Epoch 17, Batch 900, Loss: 2.2942824268341067\n",
            "Epoch 17, Accuracy on Training dataset: 0.10923333333333334\n",
            "Epoch 18, Batch 100, Loss: 2.2943588089942932\n",
            "Epoch 18, Batch 200, Loss: 2.2926970386505126\n",
            "Epoch 18, Batch 300, Loss: 2.2949226021766664\n",
            "Epoch 18, Batch 400, Loss: 2.2944463753700255\n",
            "Epoch 18, Batch 500, Loss: 2.29411630153656\n",
            "Epoch 18, Batch 600, Loss: 2.294403324127197\n",
            "Epoch 18, Batch 700, Loss: 2.2938567066192626\n",
            "Epoch 18, Batch 800, Loss: 2.294843685626984\n",
            "Epoch 18, Batch 900, Loss: 2.294866979122162\n",
            "Epoch 18, Accuracy on Training dataset: 0.11235\n",
            "Epoch 19, Batch 100, Loss: 2.2939918637275696\n",
            "Epoch 19, Batch 200, Loss: 2.294319226741791\n",
            "Epoch 19, Batch 300, Loss: 2.293957698345184\n",
            "Epoch 19, Batch 400, Loss: 2.293204870223999\n",
            "Epoch 19, Batch 500, Loss: 2.2926361346244812\n",
            "Epoch 19, Batch 600, Loss: 2.291807568073273\n",
            "Epoch 19, Batch 700, Loss: 2.293024110794067\n",
            "Epoch 19, Batch 800, Loss: 2.2935492730140687\n",
            "Epoch 19, Batch 900, Loss: 2.2939770579338075\n",
            "Epoch 19, Accuracy on Training dataset: 0.11413333333333334\n",
            "Epoch 20, Batch 100, Loss: 2.292465422153473\n",
            "Epoch 20, Batch 200, Loss: 2.2922186875343322\n",
            "Epoch 20, Batch 300, Loss: 2.2925757002830505\n",
            "Epoch 20, Batch 400, Loss: 2.2928499484062197\n",
            "Epoch 20, Batch 500, Loss: 2.292046411037445\n",
            "Epoch 20, Batch 600, Loss: 2.293158085346222\n",
            "Epoch 20, Batch 700, Loss: 2.2936085438728333\n",
            "Epoch 20, Batch 800, Loss: 2.2914263343811037\n",
            "Epoch 20, Batch 900, Loss: 2.2917711472511293\n",
            "Epoch 20, Accuracy on Training dataset: 0.11578333333333334\n",
            "Finished Training\n",
            "Accuracy on testing dataset: 0.8739333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The third hypothesis model has a more intricate structure, featuring four fully connected layers with an increasing number of neurons in each. To prevent overfitting, a dropout rate of 0.5 is applied after the first fully connected layer. Between the layers, the ReLU activation function is used and Cross Entropy Loss serves as the loss function. The model is optimized using the Adam optimizer with a very low learning rate of 0.0000001 and undergoes training for 20 epochs.\n",
        "\n",
        "During the training, the model shows poor accuracy on the training dataset, only reaching about 11.57% at the end of the process. The high loss values suggest that the model may not be effectively learning from the training data. Upon evaluation on the testing dataset, the model attains an accuracy of around 87.39%, similar to hypothesis-1 and hypothesis-2. This indicates that despite a lower training accuracy, the model might still generalize effectively to new data.\n"
      ],
      "metadata": {
        "id": "2UK8RCxvOS1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Hypothesis-4**\n",
        " * complex with 5 fully connected layers and more neurons with reLU as activation function function\n",
        "\n",
        " * Mean Square Error(MSE) as loss function\n",
        "\n",
        " * SGD as optimizer with 15 epochs"
      ],
      "metadata": {
        "id": "-y1Iri8C7zWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture hypothesis model 4\n",
        "class Hypothesis4Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hypothesis4Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize the hypothesis model 4\n",
        "hypothesis_model_4 = Hypothesis4Model()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(hypothesis_model_4.parameters(), lr=0.01)\n",
        "\n",
        "# Train the hypothesis model 4\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    hypothesis_model_4.train()\n",
        "    running_loss = 0.0\n",
        "    total_train = 0\n",
        "    correct_train = 0\n",
        "    for i, data in enumerate(QMNIST_train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = hypothesis_model_4(inputs)\n",
        "\n",
        "        # One-hot encode the labels\n",
        "        labels_onehot = torch.zeros_like(outputs)\n",
        "        labels_onehot.scatter_(1, labels.view(-1, 1), 1.0)\n",
        "        loss = criterion(outputs, labels_onehot)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted_train = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Calculate train accuracy\n",
        "    train_accuracy = correct_train / total_train\n",
        "    print(f'Epoch {epoch + 1}, Accuracy on Training dataset: {train_accuracy}')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Evaluate the hypothesis model 4\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in QMNIST_test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on testing dataset: {correct / total}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBuLt2ce72ei",
        "outputId": "df63a900-0239-4e53-910b-6a1a59a000cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 0.09622630037367344\n",
            "Epoch 1, Batch 200, Loss: 0.09329525232315064\n",
            "Epoch 1, Batch 300, Loss: 0.09204119883477688\n",
            "Epoch 1, Batch 400, Loss: 0.0910411536693573\n",
            "Epoch 1, Batch 500, Loss: 0.09057243958115578\n",
            "Epoch 1, Batch 600, Loss: 0.09019419677555561\n",
            "Epoch 1, Batch 700, Loss: 0.08992717050015926\n",
            "Epoch 1, Batch 800, Loss: 0.08992855370044708\n",
            "Epoch 1, Batch 900, Loss: 0.08970348998904228\n",
            "Epoch 1, Accuracy on Training dataset: 0.11476666666666667\n",
            "Epoch 2, Batch 100, Loss: 0.08944518081843852\n",
            "Epoch 2, Batch 200, Loss: 0.08955859780311584\n",
            "Epoch 2, Batch 300, Loss: 0.08932636231184006\n",
            "Epoch 2, Batch 400, Loss: 0.08939350008964539\n",
            "Epoch 2, Batch 500, Loss: 0.0892482853680849\n",
            "Epoch 2, Batch 600, Loss: 0.08899448722600938\n",
            "Epoch 2, Batch 700, Loss: 0.08895489685237408\n",
            "Epoch 2, Batch 800, Loss: 0.08879904322326183\n",
            "Epoch 2, Batch 900, Loss: 0.08877838477492332\n",
            "Epoch 2, Accuracy on Training dataset: 0.17073333333333332\n",
            "Epoch 3, Batch 100, Loss: 0.08863696537911891\n",
            "Epoch 3, Batch 200, Loss: 0.08860752873122692\n",
            "Epoch 3, Batch 300, Loss: 0.08848222456872464\n",
            "Epoch 3, Batch 400, Loss: 0.08821838699281216\n",
            "Epoch 3, Batch 500, Loss: 0.08817019514739513\n",
            "Epoch 3, Batch 600, Loss: 0.08805262736976147\n",
            "Epoch 3, Batch 700, Loss: 0.08788936465978622\n",
            "Epoch 3, Batch 800, Loss: 0.08788847297430039\n",
            "Epoch 3, Batch 900, Loss: 0.08761055544018745\n",
            "Epoch 3, Accuracy on Training dataset: 0.2271\n",
            "Epoch 4, Batch 100, Loss: 0.08745626889169217\n",
            "Epoch 4, Batch 200, Loss: 0.08726918667554856\n",
            "Epoch 4, Batch 300, Loss: 0.08717312969267368\n",
            "Epoch 4, Batch 400, Loss: 0.0868256789445877\n",
            "Epoch 4, Batch 500, Loss: 0.08668154314160346\n",
            "Epoch 4, Batch 600, Loss: 0.0866651713848114\n",
            "Epoch 4, Batch 700, Loss: 0.08647859416902065\n",
            "Epoch 4, Batch 800, Loss: 0.08605223566293717\n",
            "Epoch 4, Batch 900, Loss: 0.08608242474496365\n",
            "Epoch 4, Accuracy on Training dataset: 0.2887166666666667\n",
            "Epoch 5, Batch 100, Loss: 0.08578899160027503\n",
            "Epoch 5, Batch 200, Loss: 0.08537850581109524\n",
            "Epoch 5, Batch 300, Loss: 0.08517580531537533\n",
            "Epoch 5, Batch 400, Loss: 0.08496168076992035\n",
            "Epoch 5, Batch 500, Loss: 0.08475465670228005\n",
            "Epoch 5, Batch 600, Loss: 0.08425743848085404\n",
            "Epoch 5, Batch 700, Loss: 0.0842863854765892\n",
            "Epoch 5, Batch 800, Loss: 0.08397288613021374\n",
            "Epoch 5, Batch 900, Loss: 0.08353522144258023\n",
            "Epoch 5, Accuracy on Training dataset: 0.3433333333333333\n",
            "Epoch 6, Batch 100, Loss: 0.08311869159340858\n",
            "Epoch 6, Batch 200, Loss: 0.08276404522359371\n",
            "Epoch 6, Batch 300, Loss: 0.08227186985313892\n",
            "Epoch 6, Batch 400, Loss: 0.08211769625544547\n",
            "Epoch 6, Batch 500, Loss: 0.08178047344088554\n",
            "Epoch 6, Batch 600, Loss: 0.08166417241096496\n",
            "Epoch 6, Batch 700, Loss: 0.08109502889215946\n",
            "Epoch 6, Batch 800, Loss: 0.08100636832416058\n",
            "Epoch 6, Batch 900, Loss: 0.08037519812583924\n",
            "Epoch 6, Accuracy on Training dataset: 0.38575\n",
            "Epoch 7, Batch 100, Loss: 0.07970818571746349\n",
            "Epoch 7, Batch 200, Loss: 0.07922910258173943\n",
            "Epoch 7, Batch 300, Loss: 0.07937838532030582\n",
            "Epoch 7, Batch 400, Loss: 0.07881116852164269\n",
            "Epoch 7, Batch 500, Loss: 0.0782270809262991\n",
            "Epoch 7, Batch 600, Loss: 0.07792863629758358\n",
            "Epoch 7, Batch 700, Loss: 0.07790902227163315\n",
            "Epoch 7, Batch 800, Loss: 0.07749730043113232\n",
            "Epoch 7, Batch 900, Loss: 0.07700338378548623\n",
            "Epoch 7, Accuracy on Training dataset: 0.4283\n",
            "Epoch 8, Batch 100, Loss: 0.07631853513419629\n",
            "Epoch 8, Batch 200, Loss: 0.07624253295361996\n",
            "Epoch 8, Batch 300, Loss: 0.07576310388743877\n",
            "Epoch 8, Batch 400, Loss: 0.07562280364334584\n",
            "Epoch 8, Batch 500, Loss: 0.07514426231384277\n",
            "Epoch 8, Batch 600, Loss: 0.0745489864051342\n",
            "Epoch 8, Batch 700, Loss: 0.07454923376441001\n",
            "Epoch 8, Batch 800, Loss: 0.07409213505685329\n",
            "Epoch 8, Batch 900, Loss: 0.07391764871776103\n",
            "Epoch 8, Accuracy on Training dataset: 0.46563333333333334\n",
            "Epoch 9, Batch 100, Loss: 0.07324450351297855\n",
            "Epoch 9, Batch 200, Loss: 0.07305576890707016\n",
            "Epoch 9, Batch 300, Loss: 0.07318755365908146\n",
            "Epoch 9, Batch 400, Loss: 0.07240136854350566\n",
            "Epoch 9, Batch 500, Loss: 0.07193729311227798\n",
            "Epoch 9, Batch 600, Loss: 0.07188366696238518\n",
            "Epoch 9, Batch 700, Loss: 0.07162595853209495\n",
            "Epoch 9, Batch 800, Loss: 0.07074453793466091\n",
            "Epoch 9, Batch 900, Loss: 0.07096711330115796\n",
            "Epoch 9, Accuracy on Training dataset: 0.5001333333333333\n",
            "Epoch 10, Batch 100, Loss: 0.07057643905282021\n",
            "Epoch 10, Batch 200, Loss: 0.06984160117805004\n",
            "Epoch 10, Batch 300, Loss: 0.06996241308748723\n",
            "Epoch 10, Batch 400, Loss: 0.06978698387742042\n",
            "Epoch 10, Batch 500, Loss: 0.06910404026508331\n",
            "Epoch 10, Batch 600, Loss: 0.06875266950577498\n",
            "Epoch 10, Batch 700, Loss: 0.0690242439508438\n",
            "Epoch 10, Batch 800, Loss: 0.06845925565809012\n",
            "Epoch 10, Batch 900, Loss: 0.06770717158913613\n",
            "Epoch 10, Accuracy on Training dataset: 0.5355666666666666\n",
            "Epoch 11, Batch 100, Loss: 0.0673161867260933\n",
            "Epoch 11, Batch 200, Loss: 0.06713782873004676\n",
            "Epoch 11, Batch 300, Loss: 0.06726647414267063\n",
            "Epoch 11, Batch 400, Loss: 0.06651772886514663\n",
            "Epoch 11, Batch 500, Loss: 0.06647533968091011\n",
            "Epoch 11, Batch 600, Loss: 0.06597960684448481\n",
            "Epoch 11, Batch 700, Loss: 0.065662442445755\n",
            "Epoch 11, Batch 800, Loss: 0.06558084283024072\n",
            "Epoch 11, Batch 900, Loss: 0.06519975841045379\n",
            "Epoch 11, Accuracy on Training dataset: 0.5675666666666667\n",
            "Epoch 12, Batch 100, Loss: 0.06461192715913057\n",
            "Epoch 12, Batch 200, Loss: 0.06480930995196105\n",
            "Epoch 12, Batch 300, Loss: 0.06446112234145403\n",
            "Epoch 12, Batch 400, Loss: 0.0631943031400442\n",
            "Epoch 12, Batch 500, Loss: 0.06361401706933975\n",
            "Epoch 12, Batch 600, Loss: 0.06370184324681759\n",
            "Epoch 12, Batch 700, Loss: 0.06292292263358831\n",
            "Epoch 12, Batch 800, Loss: 0.06203499387949705\n",
            "Epoch 12, Batch 900, Loss: 0.06181478831917048\n",
            "Epoch 12, Accuracy on Training dataset: 0.5963166666666667\n",
            "Epoch 13, Batch 100, Loss: 0.06216481622308492\n",
            "Epoch 13, Batch 200, Loss: 0.06153925735503435\n",
            "Epoch 13, Batch 300, Loss: 0.06160375043749809\n",
            "Epoch 13, Batch 400, Loss: 0.06161329545080662\n",
            "Epoch 13, Batch 500, Loss: 0.06066776696592569\n",
            "Epoch 13, Batch 600, Loss: 0.0608708605915308\n",
            "Epoch 13, Batch 700, Loss: 0.06017080195248127\n",
            "Epoch 13, Batch 800, Loss: 0.0606631775572896\n",
            "Epoch 13, Batch 900, Loss: 0.059173258654773235\n",
            "Epoch 13, Accuracy on Training dataset: 0.61985\n",
            "Epoch 14, Batch 100, Loss: 0.05985833656042814\n",
            "Epoch 14, Batch 200, Loss: 0.05946288876235485\n",
            "Epoch 14, Batch 300, Loss: 0.05859239090234041\n",
            "Epoch 14, Batch 400, Loss: 0.05817356403917074\n",
            "Epoch 14, Batch 500, Loss: 0.05904643535614014\n",
            "Epoch 14, Batch 600, Loss: 0.058252780549228195\n",
            "Epoch 14, Batch 700, Loss: 0.0581316664069891\n",
            "Epoch 14, Batch 800, Loss: 0.05745501425117254\n",
            "Epoch 14, Batch 900, Loss: 0.05760345172137022\n",
            "Epoch 14, Accuracy on Training dataset: 0.6384833333333333\n",
            "Epoch 15, Batch 100, Loss: 0.057058713622391226\n",
            "Epoch 15, Batch 200, Loss: 0.05697341032326222\n",
            "Epoch 15, Batch 300, Loss: 0.056840107180178165\n",
            "Epoch 15, Batch 400, Loss: 0.05677108820527792\n",
            "Epoch 15, Batch 500, Loss: 0.05579609259963036\n",
            "Epoch 15, Batch 600, Loss: 0.05670562818646431\n",
            "Epoch 15, Batch 700, Loss: 0.055872932747006415\n",
            "Epoch 15, Batch 800, Loss: 0.05556514382362366\n",
            "Epoch 15, Batch 900, Loss: 0.05568869914859533\n",
            "Epoch 15, Accuracy on Training dataset: 0.6551166666666667\n",
            "Finished Training\n",
            "Accuracy on testing dataset: 0.8739333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hypothesis-4 model consists of five fully connected layers, with a gradual decrease in the number of neurons and ReLU activation functions applied between them. A dropout rate of 0.5 is used after the first and third fully connected layers for regularization. The model employs Mean Square Error as the loss function, and it is optimized using SGD with a learning rate set to 0.01. The training process involves 15 epochs.\n",
        "\n",
        "Throughout the training, the model's accuracy gradually improves on the training dataset, reaching about 65.51% at the end of the process. The loss values consistently decrease as the training goes on, showing that the model is effectively learning and optimizing its parameters. Upon testing with the dataset, the model shows an accuracy of about 87.39%, matching other hypotheses. This indicates that the model is performing well on new data, even though it was trained using a different loss function and optimizer."
      ],
      "metadata": {
        "id": "Mb4hpRIhOatM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GitHub Link**\n",
        "\n",
        "https://github.com/ManishaLagisetty/Deep_Learning_DATA255"
      ],
      "metadata": {
        "id": "nIPdPUYNeGAI"
      }
    }
  ]
}