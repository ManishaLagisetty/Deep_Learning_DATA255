{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAYd2SOypHcX"
   },
   "source": [
    "### **Part -1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tk8wZl5o-ki"
   },
   "source": [
    "**Using YoloV5 as pre-trained model for my street-scene part-1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRw0u3SaZg1H",
    "outputId": "cbbfe41d-d1c9-4401-d3b7-f4bf05ba4ee5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.1.0+cu121 CPU\n",
      "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.5/107.7 GB disk)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics==8.0.20\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "12CW-1s4ZpmI"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HViy6k98Z7jx",
    "outputId": "26c6d405-8cd0-4d77-da74-8184a75320b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BC-rwFcCZ-rr"
   },
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6HYno3DaB0A",
    "outputId": "0151772c-7481-4999-8d95-f1ecabb4e087"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v7.0-287-g574331f9 Python-3.10.12 torch-2.1.0+cu121 CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 26.5/107.7 GB disk)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ v7.0-287-g574331f9 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "#cloning from github\n",
    "!git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt\n",
    "\n",
    "import utils\n",
    "display = utils.notebook_init()\n",
    "\n",
    "import torch\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OBmbfARjNkq",
    "outputId": "aa5115dd-d01a-4e7c-ca51-12a519562742"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ v7.0-287-g574331f9 Python-3.10.12 torch-2.1.0+cu121 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:34<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video is successfully created in the drive : /content/drive/MyDrive/PART_1_DATASET/train_annotated_video.mp4\n"
     ]
    }
   ],
   "source": [
    "#loding YOLOv5 pre-trained model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# my drive directories\n",
    "\n",
    " #contains my frames and annotated files\n",
    "main_folder = '/content/drive/MyDrive/PART_1_DATASET/TRAIN'\n",
    "\n",
    "#creating a folder to store annotated frames\n",
    "annotated_folder = '/content/drive/MyDrive/PART_1_DATASET/train_annotated_frames'\n",
    "\n",
    "#output video path\n",
    "output_video = '/content/drive/MyDrive/PART_1_DATASET/train_annotated_video.mp4'\n",
    "\n",
    "#prrcessing each frame\n",
    "for frame_file in tqdm(sorted(Path(main_folder).glob('*.jpg'))):\n",
    "    frame_name = frame_file.stem\n",
    "    frame_path = str(frame_file)\n",
    "\n",
    "    #performing inference on the frame\n",
    "    results = model(frame_path)\n",
    "\n",
    "    #drawing bounding boxes\n",
    "    annotated_frame = results.render()[0]\n",
    "\n",
    "    #saving the annotated frames\n",
    "    output_path = Path(annotated_folder) / f'{frame_name}.jpg'\n",
    "    cv2.imwrite(str(output_path), annotated_frame)\n",
    "\n",
    "\n",
    "#setting the annotated frames into the video\n",
    "frames = [cv2.imread(str(frame)) for frame in sorted(Path(annotated_folder).glob('*'))]\n",
    "frame_height, frame_width, _ = frames[0].shape\n",
    "out = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'MP4V'), 10, (frame_width, frame_height))\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "out.release()\n",
    "\n",
    "print(\"Video is successfully created in the drive :\", output_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqQ8Dx0juru0"
   },
   "source": [
    "### **Saving the trained model, so that we can use it to inference on test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p011KqlZtkvK"
   },
   "outputs": [],
   "source": [
    "#saving the model\n",
    "torch.save(model, '/content/drive/MyDrive/PART_1_DATASET/train_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTl_tEZnvBo_"
   },
   "source": [
    "**Loading the saved model to inference on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5r2q5hft1Pm",
    "outputId": "135d8e0f-447c-461e-9106-35d2a1ce97e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:10<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video created successfully: /content/drive/MyDrive/PART_1_DATASET/test_annotated_video.mp4\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/content/drive/MyDrive/PART_1_DATASET/train_model.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "#contains my frames and annotated files\n",
    "main_folder_1 = '/content/drive/MyDrive/PART_1_DATASET/TEST'\n",
    "\n",
    "#creating a folder to store annotated frames\n",
    "annotated_folder_1 = '/content/drive/MyDrive/PART_1_DATASET/test_annotated_frames'\n",
    "\n",
    "#output video path\n",
    "output_video_1 = '/content/drive/MyDrive/PART_1_DATASET/test_annotated_video.mp4'  # Output video path\n",
    "\n",
    "# Process each frame\n",
    "for frame_file in tqdm(sorted(Path(main_folder_1).glob('*.jpg'))):  # Only process JPEG images\n",
    "    frame_name = frame_file.stem\n",
    "    frame_path = str(frame_file)\n",
    "\n",
    "    # Perform inference on the frame\n",
    "    results = model(frame_path)\n",
    "\n",
    "    # Draw bounding boxes around detected vehicles\n",
    "    annotated_frame = results.render()[0]\n",
    "\n",
    "    # Save the annotated frame\n",
    "    output_path = Path(annotated_folder_1) / f'{frame_name}.jpg'\n",
    "    cv2.imwrite(str(output_path), annotated_frame)\n",
    "\n",
    "# Compile annotated frames into a video\n",
    "frames = [cv2.imread(str(frame)) for frame in sorted(Path(annotated_folder_1).glob('*'))]\n",
    "frame_height, frame_width, _ = frames[0].shape\n",
    "out = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'MP4V'), 10, (frame_width, frame_height))\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "out.release()\n",
    "\n",
    "print(\"Video created successfully:\", output_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drive Link ---https://drive.google.com/drive/u/0/folders/1dv156su-xNFShUKSgvIeYY3OdLZuGA-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
